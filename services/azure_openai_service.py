import asyncio
import aiohttp
import json
from typing import List, Dict, Any, Optional, AsyncGenerator

class OpenAIService:
    """OpenAI æœåŠ¡ç±»"""

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://llm.onerouter.pro/v1",
        model: str = "gemini-3-flash-preview"
    ):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

    async def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ OpenAI Chat Completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            stream: æ˜¯å¦æµå¼è¿”å›

        Returns:
            API å“åº”
        """
        url = f"{self.base_url}/chat/completions"

        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream,
            "extra_body": {
                "reasoning": {
                    "max_tokens": 1
                }
            }
        }

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, headers=self.headers, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result
                    else:
                        error_text = await response.text()
                        raise Exception(f"OpenAI API é”™è¯¯: {response.status} - {error_text}")

            except aiohttp.ClientError as e:
                raise Exception(f"è¯·æ±‚ OpenAI API æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, Any]],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼è°ƒç”¨ OpenAI Chat Completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°

        Yields:
            æµå¼å“åº”ç‰‡æ®µ
        """
        url = f"{self.base_url}/chat/completions"

        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True,
            "extra_body": {
                "reasoning": {
                    "max_tokens": 1
                }
            }
        }

        async with aiohttp.ClientSession() as session:
            try:
                # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
                request_start_time = asyncio.get_event_loop().time()
                first_chunk_time = None
                chunk_count = 0

                async with session.post(url, headers=self.headers, json=payload) as response:
                    if response.status == 200:
                        async for line in response.content:
                            line = line.decode('utf-8').strip()
                            if line.startswith('data: '):
                                data = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                                if data == '[DONE]':
                                    break
                                try:
                                    chunk = json.loads(data)

                                    # è®°å½•ç¬¬ä¸€ä¸ªchunkçš„æ—¶é—´
                                    if first_chunk_time is None:
                                        first_chunk_time = asyncio.get_event_loop().time()
                                        time_to_first_output = (first_chunk_time - request_start_time) * 1000
                                        print(f"\n{'='*80}")
                                        print(f"â±ï¸  Gemini API æ—¶é—´ç»Ÿè®¡")
                                        print(f"ğŸ“¥ è¯·æ±‚ Gemini â†’ ğŸ“¤ é¦–ä¸ªè¾“å‡º: {time_to_first_output:.2f}ms")
                                        print(f"{'='*80}\n")

                                    chunk_count += 1
                                    yield chunk
                                except json.JSONDecodeError:
                                    continue
                    else:
                        error_text = await response.text()
                        raise Exception(f"OpenAI API é”™è¯¯: {response.status} - {error_text}")

            except aiohttp.ClientError as e:
                raise Exception(f"è¯·æ±‚ OpenAI API æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {str(e)}")


class AzureOpenAIService:
    """Azure OpenAI æœåŠ¡ç±»"""

    def __init__(
        self,
        endpoint: str,
        api_key: str,
        api_version: str = "2024-02-15-preview",
        deployment_name: str = "gpt-4.1"
    ):
        self.endpoint = endpoint.rstrip('/')
        self.api_key = api_key
        self.api_version = api_version
        self.deployment_name = deployment_name
        self.base_url = f"{self.endpoint}/openai/deployments/{self.deployment_name}"

    async def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ Azure OpenAI Chat Completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            stream: æ˜¯å¦æµå¼è¿”å›

        Returns:
            API å“åº”
        """
        url = f"{self.base_url}/chat/completions?api-version={self.api_version}"

        headers = {
            "Content-Type": "application/json",
            "api-key": self.api_key
        }

        payload = {
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream
        }

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result
                    else:
                        error_text = await response.text()
                        raise Exception(f"Azure OpenAI API é”™è¯¯: {response.status} - {error_text}")

            except aiohttp.ClientError as e:
                raise Exception(f"è¯·æ±‚ Azure OpenAI API æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, Any]],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼è°ƒç”¨ Azure OpenAI Chat Completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°

        Yields:
            æµå¼å“åº”ç‰‡æ®µ
        """
        url = f"{self.base_url}/chat/completions?api-version={self.api_version}"

        headers = {
            "Content-Type": "application/json",
            "api-key": self.api_key
        }

        payload = {
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True
        }

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        async for line in response.content:
                            line = line.decode('utf-8').strip()
                            if line.startswith('data: '):
                                data = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                                if data == '[DONE]':
                                    break
                                try:
                                    chunk = json.loads(data)
                                    yield chunk
                                except json.JSONDecodeError:
                                    continue
                    else:
                        error_text = await response.text()
                        raise Exception(f"Azure OpenAI API é”™è¯¯: {response.status} - {error_text}")

            except aiohttp.ClientError as e:
                raise Exception(f"è¯·æ±‚ Azure OpenAI API æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {str(e)}")


class DoubaoService:
    """è±†åŒ…æœåŠ¡ç±» - å­—èŠ‚è·³åŠ¨ AI åŠ©æ‰‹"""

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://ark.cn-beijing.volces.com",
        model: str = "doubao-lite-4k",
        timeout: int = 30
    ):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.timeout = timeout
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

    async def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        max_tokens: int = 4000,
        temperature: float = 0.7,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨è±†åŒ… Chat Completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            stream: æ˜¯å¦æµå¼è¿”å›

        Returns:
            API å“åº”
        """
        url = f"{self.base_url}/chat/completions"

        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream,
            "reasoning_effort": "minimal"
        }

        timeout = aiohttp.ClientTimeout(total=self.timeout)

        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                async with session.post(url, headers=self.headers, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result
                    else:
                        error_text = await response.text()
                        raise Exception(f"è±†åŒ… API é”™è¯¯: {response.status} - {error_text}")

            except aiohttp.ClientError as e:
                raise Exception(f"è¯·æ±‚è±†åŒ… API æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {str(e)}")

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, Any]],
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼è°ƒç”¨è±†åŒ… Chat Completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°

        Yields:
            æµå¼å“åº”ç‰‡æ®µ
        """
        url = f"{self.base_url}/chat/completions"

        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True,
            "reasoning_effort": "minimal"
        }

        timeout = aiohttp.ClientTimeout(total=self.timeout)

        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
                request_start_time = asyncio.get_event_loop().time()
                first_chunk_time = None
                chunk_count = 0

                async with session.post(url, headers=self.headers, json=payload) as response:
                    if response.status == 200:
                        async for line in response.content:
                            line = line.decode('utf-8').strip()
                            if line.startswith('data: '):
                                data = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                                if data == '[DONE]':
                                    break
                                try:
                                    chunk = json.loads(data)

                                    # è®°å½•ç¬¬ä¸€ä¸ªchunkçš„æ—¶é—´
                                    if first_chunk_time is None:
                                        first_chunk_time = asyncio.get_event_loop().time()
                                        time_to_first_output = (first_chunk_time - request_start_time) * 1000
                                        print(f"\n{'='*80}")
                                        print(f"â±ï¸ è±†åŒ… API æ—¶é—´ç»Ÿè®¡")
                                        print(f"ğŸ“¥ è¯·æ±‚è±†åŒ… â†’ ğŸ“¤ é¦–ä¸ªè¾“å‡º: {time_to_first_output:.2f}ms")
                                        print(f"{'='*80}\n")

                                    chunk_count += 1
                                    yield chunk
                                except json.JSONDecodeError:
                                    continue
                    else:
                        error_text = await response.text()
                        raise Exception(f"è±†åŒ… API é”™è¯¯: {response.status} - {error_text}")

            except aiohttp.ClientError as e:
                raise Exception(f"è¯·æ±‚è±†åŒ… API æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {str(e)}")