#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•åŸºç±»æ¨¡å—
å®šä¹‰äº†æ‰€æœ‰æ¨¡åž‹æµ‹è¯•å™¨çš„åŸºæœ¬æŽ¥å£å’Œé€šç”¨åŠŸèƒ½
"""

import json
import time
import asyncio
import aiohttp
import statistics
import numpy as np
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import logging


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æžœæ•°æ®ç±»"""

    model_name: str
    prompt_id: int
    prompt_type: str
    response_time_ms: float
    input_tokens: int
    output_tokens: int
    tokens_per_second: float
    success: bool
    error_message: Optional[str] = None
    timestamp: str = None
    raw_response: Optional[Dict] = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()


class BaseTester(ABC):
    """
    æµ‹è¯•å™¨æŠ½è±¡åŸºç±»

    å®šä¹‰äº†æ‰€æœ‰æ¨¡åž‹æµ‹è¯•å™¨å¿…é¡»å®žçŽ°çš„æŽ¥å£å’Œé€šç”¨åŠŸèƒ½
    """

    def __init__(self, model_name: str, api_key: str, **kwargs):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨

        Args:
            model_name: æ¨¡åž‹åç§°
            api_key: APIå¯†é’¥
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
        """
        self.model_name = model_name
        self.api_key = api_key
        self.base_url = kwargs.get('base_url', '')
        self.headers = kwargs.get('headers', {})
        self.model = kwargs.get('model', model_name)
        self.timeout = kwargs.get('timeout', 30)
        self.results: List[TestResult] = []
        self.logger = logging.getLogger(self.__class__.__name__)

    @abstractmethod
    async def chat_completion(self, messages: List[Dict]) -> Dict:
        """
        å‘é€èŠå¤©è¯·æ±‚å¹¶è¿”å›žç»“æžœ

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            è§£æžåŽçš„å“åº”å­—å…¸
        """
        pass

    @abstractmethod
    def extract_content(self, response: Dict) -> str:
        """
        ä»ŽAPIå“åº”ä¸­æå–æ–‡æœ¬å†…å®¹

        Args:
            response: APIè¿”å›žçš„åŽŸå§‹å“åº”

        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        pass

    def get_api_url(self) -> str:
        """
        èŽ·å–APIç«¯ç‚¹URL

        å­ç±»å¿…é¡»é‡å†™æ­¤æ–¹æ³•

        Returns:
            APIç«¯ç‚¹URL
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®žçŽ° get_api_url æ–¹æ³•")

    def estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            ä¼°ç®—çš„tokenæ•°é‡
        """
        if not text:
            return 0

        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦çº¦2å­—ç¬¦/Tokenï¼Œè‹±æ–‡å­—ç¬¦çº¦4å­—ç¬¦/Token
        chinese_chars = sum(1 for c in text if ord(c) > 127)
        english_chars = len(text) - chinese_chars

        return int(chinese_chars / 2 + english_chars / 4)

    def parse_complete_prompt(self, complete_prompt_str: str) -> List[Dict]:
        """
        è§£æžcomplete_promptå­—ç¬¦ä¸²ä¸ºæ¶ˆæ¯åˆ—è¡¨

        Args:
            complete_prompt_str: JSONå­—ç¬¦ä¸²æ ¼å¼çš„prompt

        Returns:
            è§£æžåŽçš„æ¶ˆæ¯åˆ—è¡¨
        """
        try:
            messages = json.loads(complete_prompt_str)

            # æå–æœ€åŽä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_messages = [msg for msg in messages if msg.get('role') == 'user']
            if user_messages:
                last_message = user_messages[-1]

                # å¤„ç†å¤åˆå†…å®¹ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
                if isinstance(last_message.get('content'), list):
                    text_content = ""
                    for item in last_message['content']:
                        if item.get('type') == 'input_text':
                            text_content += item.get('text', '')
                    final_message = {"role": "user", "content": text_content}
                else:
                    final_message = last_message

                # æž„é€ ç®€åŒ–çš„å¯¹è¯ï¼ˆåªä¿ç•™ç³»ç»Ÿæç¤ºå’Œæœ€åŽç”¨æˆ·æ¶ˆæ¯ï¼‰
                simplified_messages = []
                for msg in messages:
                    if msg.get('role') == 'system':
                        simplified_messages.append(msg)
                simplified_messages.append(final_message)
                return simplified_messages

            return [{"role": "user", "content": "Hello"}]
        except Exception as e:
            self.logger.warning(f"è§£æžpromptå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ¶ˆæ¯")
            return [{"role": "user", "content": complete_prompt_str[:100]}]

    async def test_single_prompt(self, prompt: Dict) -> TestResult:
        """
        æµ‹è¯•å•ä¸ªprompt

        Args:
            prompt: promptæ•°æ®

        Returns:
            æµ‹è¯•ç»“æžœ
        """
        prompt_id = prompt.get('id', 0)
        prompt_type = prompt.get('type', 'text')

        # è§£æžæ¶ˆæ¯å¹¶è®¡ç®—è¾“å…¥tokenæ•°
        try:
            messages = self.parse_complete_prompt(prompt['complete_prompt'])
            input_text = json.dumps(messages, ensure_ascii=False)
            input_tokens = self.estimate_tokens(input_text)
        except Exception as e:
            self.logger.error(f"è§£æžpromptå¤±è´¥ (ID: {prompt_id}): {e}")
            input_tokens = self.estimate_tokens(str(prompt))

        start_time = time.time()

        try:
            # å‘é€è¯·æ±‚
            response = await self.chat_completion(messages)

            end_time = time.time()
            response_time_ms = (end_time - start_time) * 1000

            # æå–å“åº”æ–‡æœ¬
            content = self.extract_content(response)

            # è®¡ç®—è¾“å‡ºtokenæ•°
            output_tokens = self.estimate_tokens(content)

            # è®¡ç®—tokenç”Ÿæˆé€Ÿåº¦
            if response_time_ms > 0:
                tokens_per_second = (output_tokens / response_time_ms) * 1000
            else:
                tokens_per_second = 0

            return TestResult(
                model_name=self.model_name,
                prompt_id=prompt_id,
                prompt_type=prompt_type,
                response_time_ms=response_time_ms,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                tokens_per_second=tokens_per_second,
                success=True,
                raw_response=response
            )

        except Exception as e:
            end_time = time.time()
            response_time_ms = (end_time - start_time) * 1000

            self.logger.error(f"æµ‹è¯•å¤±è´¥ (ID: {prompt_id}): {e}")

            return TestResult(
                model_name=self.model_name,
                prompt_id=prompt_id,
                prompt_type=prompt_type,
                response_time_ms=response_time_ms,
                input_tokens=input_tokens,
                output_tokens=0,
                tokens_per_second=0,
                success=False,
                error_message=str(e)
            )

    async def run_test(self,
                       prompts: List[Dict],
                       delay_between_requests: float = 2.0) -> List[TestResult]:
        """
        è¿è¡Œæ¨¡åž‹æµ‹è¯•

        Args:
            prompts: promptåˆ—è¡¨
            delay_between_requests: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

        Returns:
            æµ‹è¯•ç»“æžœåˆ—è¡¨
        """
        self.logger.info(f"ðŸš€ å¼€å§‹æµ‹è¯•æ¨¡åž‹: {self.model_name}")
        self.logger.info(f"ðŸ“Š æµ‹è¯•promptsæ•°é‡: {len(prompts)}")

        # ä¸ºæ¯ä¸ªpromptæ·»åŠ æ—¶é—´æˆ³
        for prompt in prompts:
            prompt['timestamp'] = datetime.now().isoformat()

        results = []
        for i, prompt in enumerate(prompts, 1):
            prompt_start_time = datetime.now()
            self.logger.info(f"[{prompt_start_time.strftime('%Y-%m-%d %H:%M:%S')}] [{i}/{len(prompts)}] æµ‹è¯•prompt ID: {prompt['id']}")

            result = await self.test_single_prompt(prompt)
            results.append(result)
            self.results.append(result)

            # å»¶è¿Ÿé¿å…APIé™åˆ¶
            if delay_between_requests > 0:
                await asyncio.sleep(delay_between_requests)

            # æ˜¾ç¤ºç»“æžœ
            if result.success:
                self.logger.info(
                    f"    âœ… {result.response_time_ms:.0f}ms | "
                    f"{result.output_tokens}tokens | "
                    f"{result.tokens_per_second:.1f} tokens/s | "
                    f"æ—¶é—´æˆ³: {result.timestamp}"
                )
            else:
                error_msg = result.error_message[:50] + "..." if len(result.error_message) > 50 else result.error_message
                self.logger.error(f"    âŒ é”™è¯¯: {error_msg} | æ—¶é—´æˆ³: {result.timestamp}")

        return results

    def calculate_statistics(self) -> Dict[str, Any]:
        """
        è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡

        Returns:
            ç»Ÿè®¡ç»“æžœå­—å…¸
        """
        model_results = [r for r in self.results if r.success]

        if not model_results:
            return {"error": f"æ¨¡åž‹ {self.model_name} æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æžœ"}

        # æ•´ä½“ç»Ÿè®¡
        response_times = [r.response_time_ms for r in model_results]
        output_tokens = [r.output_tokens for r in model_results]
        tokens_per_second = [r.tokens_per_second for r in model_results]

        # æŒ‰ç±»åž‹åˆ†ç±»ç»Ÿè®¡
        text_results = [r for r in model_results if r.prompt_type == 'text']
        image_results = [r for r in model_results if r.prompt_type == 'image']

        stats = {
            "model_name": self.model_name,
            "total_tests": len(self.results),
            "successful_tests": len(model_results),
            "failed_tests": len(self.results) - len(model_results),

            "response_time_ms": {
                "mean": round(statistics.mean(response_times), 2),
                "median": round(statistics.median(response_times), 2),
                "min": round(min(response_times), 2),
                "max": round(max(response_times), 2),
                "p80": round(np.percentile(response_times, 80), 2),
                "p90": round(np.percentile(response_times, 90), 2),
                "p99": round(np.percentile(response_times, 99), 2),
            },

            "output_tokens": {
                "mean": round(statistics.mean(output_tokens), 2),
                "median": round(statistics.median(output_tokens), 2),
                "min": min(output_tokens),
                "max": max(output_tokens),
            },

            "tokens_per_second": {
                "mean": round(statistics.mean(tokens_per_second), 2),
                "median": round(statistics.median(tokens_per_second), 2),
                "min": round(min(tokens_per_second), 2),
                "max": round(max(tokens_per_second), 2),
                "p80": round(np.percentile(tokens_per_second, 80), 2),
                "p90": round(np.percentile(tokens_per_second, 90), 2),
                "p99": round(np.percentile(tokens_per_second, 99), 2),
            }
        }

        # æ·»åŠ æ–‡æœ¬ç±»åž‹ç»Ÿè®¡
        if text_results:
            text_response_times = [r.response_time_ms for r in text_results]
            text_output_tokens = [r.output_tokens for r in text_results]
            text_tokens_per_second = [r.tokens_per_second for r in text_results]

            stats["text_type"] = {
                "count": len(text_results),
                "response_time_ms": {
                    "mean": round(statistics.mean(text_response_times), 2),
                    "median": round(statistics.median(text_response_times), 2),
                    "min": round(min(text_response_times), 2),
                    "max": round(max(text_response_times), 2),
                    "p80": round(np.percentile(text_response_times, 80), 2),
                    "p90": round(np.percentile(text_response_times, 90), 2),
                    "p99": round(np.percentile(text_response_times, 99), 2),
                },
                "output_tokens": {
                    "mean": round(statistics.mean(text_output_tokens), 2),
                    "median": round(statistics.median(text_output_tokens), 2),
                    "min": min(text_output_tokens),
                    "max": max(text_output_tokens),
                },
                "tokens_per_second": {
                    "mean": round(statistics.mean(text_tokens_per_second), 2),
                    "median": round(statistics.median(text_tokens_per_second), 2),
                    "min": round(min(text_tokens_per_second), 2),
                    "max": round(max(text_tokens_per_second), 2),
                    "p80": round(np.percentile(text_tokens_per_second, 80), 2),
                    "p90": round(np.percentile(text_tokens_per_second, 90), 2),
                    "p99": round(np.percentile(text_tokens_per_second, 99), 2),
                }
            }

        # æ·»åŠ å›¾ç‰‡ç±»åž‹ç»Ÿè®¡
        if image_results:
            image_response_times = [r.response_time_ms for r in image_results]
            image_output_tokens = [r.output_tokens for r in image_results]
            image_tokens_per_second = [r.tokens_per_second for r in image_results]

            stats["image_type"] = {
                "count": len(image_results),
                "response_time_ms": {
                    "mean": round(statistics.mean(image_response_times), 2),
                    "median": round(statistics.median(image_response_times), 2),
                    "min": round(min(image_response_times), 2),
                    "max": round(max(image_response_times), 2),
                    "p80": round(np.percentile(image_response_times, 80), 2),
                    "p90": round(np.percentile(image_response_times, 90), 2),
                    "p99": round(np.percentile(image_response_times, 99), 2),
                },
                "output_tokens": {
                    "mean": round(statistics.mean(image_output_tokens), 2),
                    "median": round(statistics.median(image_output_tokens), 2),
                    "min": min(image_output_tokens),
                    "max": max(image_output_tokens),
                },
                "tokens_per_second": {
                    "mean": round(statistics.mean(image_tokens_per_second), 2),
                    "median": round(statistics.median(image_tokens_per_second), 2),
                    "min": round(min(image_tokens_per_second), 2),
                    "max": round(max(image_tokens_per_second), 2),
                    "p80": round(np.percentile(image_tokens_per_second, 80), 2),
                    "p90": round(np.percentile(image_tokens_per_second, 90), 2),
                    "p99": round(np.percentile(image_tokens_per_second, 99), 2),
                }
            }

        # æ·»åŠ å›¾ç‰‡ vs æ–‡æœ¬å¯¹æ¯”
        if text_results and image_results:
            text_avg_time = statistics.mean([r.response_time_ms for r in text_results])
            image_avg_time = statistics.mean([r.response_time_ms for r in image_results])
            text_avg_speed = statistics.mean([r.tokens_per_second for r in text_results])
            image_avg_speed = statistics.mean([r.tokens_per_second for r in image_results])

            time_diff_pct = ((image_avg_time - text_avg_time) / text_avg_time) * 100 if text_avg_time > 0 else 0
            speed_diff_pct = ((text_avg_speed - image_avg_speed) / image_avg_speed) * 100 if image_avg_speed > 0 else 0

            stats["image_vs_text_comparison"] = {
                "response_time": {
                    "text_mean_ms": round(text_avg_time, 2),
                    "image_mean_ms": round(image_avg_time, 2),
                    "image_vs_text_diff_pct": round(time_diff_pct, 2),
                    "analysis": "å›¾ç‰‡æ¯”æ–‡æœ¬æ…¢" if time_diff_pct > 0 else "å›¾ç‰‡æ¯”æ–‡æœ¬å¿«" if time_diff_pct < 0 else "å›¾ç‰‡å’Œæ–‡æœ¬é€Ÿåº¦ç›¸åŒ"
                },
                "tokens_per_second": {
                    "text_mean": round(text_avg_speed, 2),
                    "image_mean": round(image_avg_speed, 2),
                    "text_vs_image_diff_pct": round(speed_diff_pct, 2),
                    "analysis": "æ–‡æœ¬æ¯”å›¾ç‰‡å¿«" if speed_diff_pct > 0 else "æ–‡æœ¬æ¯”å›¾ç‰‡æ…¢" if speed_diff_pct < 0 else "æ–‡æœ¬å’Œå›¾ç‰‡é€Ÿåº¦ç›¸åŒ"
                }
            }

        return stats

    def print_statistics(self):
        """
        æ‰“å°ç»Ÿè®¡ç»“æžœ
        """
        stats = self.calculate_statistics()

        if "error" in stats:
            print(f"\nâŒ {stats['error']}")
            return

        print(f"\nðŸ“Š {stats['model_name']} æµ‹è¯•ç»Ÿè®¡ç»“æžœ:")
        print(f"  æ€»æµ‹è¯•: {stats['total_tests']} | æˆåŠŸ: {stats['successful_tests']} | å¤±è´¥: {stats['failed_tests']}")

        rt = stats['response_time_ms']
        print(f"\nâ±ï¸ å“åº”æ—¶é—´ (ms):")
        print(f"  å‡å€¼: {rt['mean']} | ä¸­ä½æ•°: {rt['median']} | æœ€å°: {rt['min']} | æœ€å¤§: {rt['max']}")
        print(f"  P80: {rt['p80']} | P90: {rt['p90']} | P99: {rt['p99']}")

        tps = stats['tokens_per_second']
        print(f"\nðŸš€ Tokenç”Ÿæˆé€Ÿåº¦ (tokens/s):")
        print(f"  å‡å€¼: {tps['mean']} | ä¸­ä½æ•°: {tps['median']} | æœ€å°: {tps['min']} | æœ€å¤§: {tps['max']}")
        print(f"  P80: {tps['p80']} | P90: {tps['p90']} | P99: {tps['p99']}")

        tokens = stats['output_tokens']
        print(f"\nðŸ“ è¾“å‡ºTokenæ•°é‡:")
        print(f"  å‡å€¼: {tokens['mean']} | ä¸­ä½æ•°: {tokens['median']} | æœ€å°: {tokens['min']} | æœ€å¤§: {tokens['max']}")

        # æ‰“å°æ–‡æœ¬ç±»åž‹ç»Ÿè®¡
        if "text_type" in stats:
            text_stats = stats["text_type"]
            print(f"\nðŸ“ æ–‡æœ¬ç±»åž‹ (æ— å›¾ç‰‡) - {text_stats['count']}ä¸ªæµ‹è¯•:")
            rt = text_stats['response_time_ms']
            print(f"  å“åº”æ—¶é—´: å‡å€¼{rt['mean']}ms | ä¸­ä½æ•°{rt['median']}ms | æœ€å°{rt['min']}ms | æœ€å¤§{rt['max']}ms")
            tps = text_stats['tokens_per_second']
            print(f"  Tokené€Ÿåº¦: å‡å€¼{tps['mean']} tok/s | ä¸­ä½æ•°{tps['median']} tok/s | æœ€å°{tps['min']} tok/s | æœ€å¤§{tps['max']} tok/s")
            tok = text_stats['output_tokens']
            print(f"  è¾“å‡ºToken: å‡å€¼{tok['mean']} | ä¸­ä½æ•°{tok['median']} | æœ€å°{tok['min']} | æœ€å¤§{tok['max']}")

        # æ‰“å°å›¾ç‰‡ç±»åž‹ç»Ÿè®¡
        if "image_type" in stats:
            image_stats = stats["image_type"]
            print(f"\nðŸ“¸ å›¾ç‰‡ç±»åž‹ (æœ‰å›¾ç‰‡) - {image_stats['count']}ä¸ªæµ‹è¯•:")
            rt = image_stats['response_time_ms']
            print(f"  å“åº”æ—¶é—´: å‡å€¼{rt['mean']}ms | ä¸­ä½æ•°{rt['median']}ms | æœ€å°{rt['min']}ms | æœ€å¤§{rt['max']}ms")
            tps = image_stats['tokens_per_second']
            print(f"  Tokené€Ÿåº¦: å‡å€¼{tps['mean']} tok/s | ä¸­ä½æ•°{tps['median']} tok/s | æœ€å°{tps['min']} tok/s | æœ€å¤§{tps['max']} tok/s")
            tok = image_stats['output_tokens']
            print(f"  è¾“å‡ºToken: å‡å€¼{tok['mean']} | ä¸­ä½æ•°{tok['median']} | æœ€å°{tok['min']} | æœ€å¤§{tok['max']}")

        # æ‰“å°å›¾ç‰‡ vs æ–‡æœ¬å¯¹æ¯”
        if "image_vs_text_comparison" in stats:
            comp = stats["image_vs_text_comparison"]
            print(f"\nðŸ“Š å›¾ç‰‡ vs æ–‡æœ¬é€Ÿåº¦å¯¹æ¯”:")
            print(f"  å“åº”æ—¶é—´: æ–‡æœ¬{comp['response_time']['text_mean_ms']}ms vs å›¾ç‰‡{comp['response_time']['image_mean_ms']}ms")
            print(f"    {comp['response_time']['analysis']} {abs(comp['response_time']['image_vs_text_diff_pct']):.1f}%")
            print(f"  Tokené€Ÿåº¦: æ–‡æœ¬{comp['tokens_per_second']['text_mean']} tok/s vs å›¾ç‰‡{comp['tokens_per_second']['image_mean']} tok/s")
            print(f"    {comp['tokens_per_second']['analysis']} {abs(comp['tokens_per_second']['text_vs_image_diff_pct']):.1f}%")

    def save_results(self, filename: str):
        """
        ä¿å­˜æµ‹è¯•ç»“æžœåˆ°æ–‡ä»¶

        Args:
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        # ä¿å­˜è¯¦ç»†ç»“æžœ
        detailed_results = [asdict(r) for r in self.results]

        output = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "model_name": self.model_name,
                "total_results": len(self.results),
                "successful_results": len([r for r in self.results if r.success]),
                "failed_results": len([r for r in self.results if not r.success]),
            },
            "detailed_results": detailed_results
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        # è®¡ç®—ç»Ÿè®¡ç»“æžœ
        statistics = self.calculate_statistics()

        # ä¿å­˜ç»Ÿè®¡ç»“æžœ
        stats_output = {
            "test_info": output["test_info"],
            "statistics": statistics
        }

        stats_filename = filename.replace('.json', '_stats.json')
        with open(stats_filename, 'w', encoding='utf-8') as f:
            json.dump(stats_output, f, ensure_ascii=False, indent=2)

        print(f"\nðŸ’¾ æµ‹è¯•ç»“æžœå·²ä¿å­˜:")
        print(f"  â€¢ è¯¦ç»†ç»“æžœ: {filename}")
        print(f"  â€¢ ç»Ÿè®¡ç»“æžœ: {stats_filename}")

        # æ‰“å°ç»Ÿè®¡ç»“æžœ
        self.print_statistics()