#!/usr/bin/env python3
"""
æµ‹è¯• Gemini API é¦–ä¸ªè¾“å‡ºæ—¶é—´ç»Ÿè®¡
"""

import asyncio
import sys
import aiohttp
import json
from services.azure_openai_service import OpenAIService
from config import settings

async def test_gemini_first_chunk_time():
    """æµ‹è¯•ä»è¯·æ±‚ Gemini åˆ°è¿”å›ç¬¬ä¸€ä¸ªè¾“å‡ºå—çš„æ—¶é—´"""

    print("="*80)
    print("ğŸ§ª æµ‹è¯• Gemini API é¦–ä¸ªè¾“å‡ºæ—¶é—´")
    print("="*80)

    # åˆ›å»º OpenAI æœåŠ¡å®ä¾‹ï¼ˆå®é™…æ˜¯ Geminiï¼‰
    openai_service = OpenAIService(
        api_key=settings.openai_api_key,
        base_url=settings.openai_base_url,
        model=settings.openai_model
    )

    messages = [
        {
            "role": "user",
            "content": "è¯·å†™ä¸€ä¸ªç®€çŸ­çš„Pythonå‡½æ•°ï¼Œè®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ"
        }
    ]

    print(f"\nğŸ“¤ å‘é€è¯·æ±‚åˆ°: {settings.openai_base_url}")
    print(f"ğŸ¤– æ¨¡å‹: {settings.openai_model}")
    print(f"ğŸ’¬ æŸ¥è¯¢: {messages[0]['content']}\n")

    try:
        print("="*80)
        print("ğŸ“¥ æ¥æ”¶ Gemini æµå¼å“åº”:")
        print("="*80)

        chunk_count = 0
        async for chunk in openai_service.chat_completion_stream(
            messages,
            max_tokens=500,
            temperature=0.7
        ):
            chunk_count += 1
            if chunk_count == 1:
                print(f"\nâœ… é¦–ä¸ªè¾“å‡ºå·²æ¥æ”¶ï¼ˆè§ä¸Šæ–¹æ—¶é—´ç»Ÿè®¡ï¼‰")
            elif chunk_count <= 5:
                # æ˜¾ç¤ºå‰å‡ ä¸ªchunkçš„å†…å®¹é¢„è§ˆ
                delta = chunk.get("choices", [{}])[0].get("delta", {})
                content = delta.get("content", "")
                if content:
                    print(f"  ğŸ“ Chunk {chunk_count}: {content[:50]}...")

        print(f"\n{'='*80}")
        print(f"âœ… æµ‹è¯•å®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—")
        print(f"{'='*80}\n")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}\n")
        return False

    return True

if __name__ == "__main__":
    result = asyncio.run(test_gemini_first_chunk_time())
    sys.exit(0 if result else 1)