#!/usr/bin/env python3
"""
OpenAI æ¥å£ç›´æ¥æµ‹è¯•è„šæœ¬

ç›´æ¥è°ƒç”¨ OpenAI APIï¼Œæ— éœ€é€šè¿‡ FastAPI æœåŠ¡å™¨ã€‚
åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ç»´æŠ¤ä½ çš„ messagesï¼Œç„¶åè¿è¡Œè„šæœ¬æŸ¥çœ‹ OpenAI çš„å“åº”ã€‚
"""

import asyncio
import json
import sys
from typing import List, Dict, Any

# å¯¼å…¥é¡¹ç›®ä¸­çš„ OpenAI æœåŠ¡
import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from services.azure_openai_service import OpenAIService


class OpenAIChatTester:
    """OpenAI èŠå¤©æ¥å£æµ‹è¯•å™¨"""

    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY", "sk-hk69mLmsHF6FfIM8cPn2Zitfk0Jca6suzwIptZymPn6h1u6x")
        self.base_url = os.getenv("OPENAI_BASE_URL", "https://llm.onerouter.pro/v1")
        self.model = os.getenv("OPENAI_MODEL", "gemini-3-flash-preview")

        print(f"ğŸ”‘ OpenAI é…ç½®:")
        print(f"   API Key: {self.api_key[:20]}...")
        print(f"   Base URL: {self.base_url}")
        print(f"   Model: {self.model}")

        # åˆå§‹åŒ– OpenAI æœåŠ¡
        self.openai_service = OpenAIService(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model
        )

    async def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        å‘é€èŠå¤©è¯·æ±‚åˆ° OpenAI

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º

        Returns:
            OpenAI API å“åº”
        """
        print("\n" + "=" * 80)
        print("ğŸš€ å‘é€è¯·æ±‚åˆ° OpenAI")
        print("=" * 80)
        print(f"ğŸ“ è¯·æ±‚å‚æ•°:")
        print(f"   Model: {self.model}")
        print(f"   Max Tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        print(f"   Stream: {stream}")
        print(f"\nğŸ“„ æ¶ˆæ¯åˆ—è¡¨:")
        for i, msg in enumerate(messages, 1):
            print(f"   {i}. [{msg['role']}] {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}")

        print("\n" + "=" * 80)
        print("ğŸ“Š OpenAI å“åº”:")
        print("=" * 80)

        try:
            if stream:
                # æµå¼è¾“å‡º
                print("ğŸ”„ æµå¼å“åº”:")
                full_content = ""
                async for chunk in self.openai_service.chat_completion_stream(
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                ):
                    print(chunk, end="", flush=True)
                    full_content += chunk

                print("\n")  # æ¢è¡Œ
                return {"choices": [{"message": {"content": full_content}}]}

            else:
                # æ™®é€šå“åº”
                response = await self.openai_service.chat_completion(
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=stream
                )

                # ç¾åŒ–è¾“å‡º
                print(json.dumps(response, indent=2, ensure_ascii=False))

                return response

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}


# =============================================================================
# åœ¨è¿™é‡Œç»´æŠ¤ä½ çš„æµ‹è¯•æ¶ˆæ¯
# =============================================================================

# OpenAI æ¶ˆæ¯æ ¼å¼ï¼š [{"role": "user", "content": "..."}]

# ç¤ºä¾‹ 1: ç®€å•å¯¹è¯
SIMPLE_MESSAGES = [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
]

# ç¤ºä¾‹ 2: å¤šè½®å¯¹è¯
MULTI_TURN_MESSAGES = [
    {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹  Python ç¼–ç¨‹"},
    {"role": "user", "content": "èƒ½è¯¦ç»†è¯´è¯´å˜é‡å’Œæ•°æ®ç±»å‹å—ï¼Ÿ"}
]

# ç¤ºä¾‹ 3: å¸¦ç³»ç»Ÿæç¤ºçš„å¯¹è¯
SYSTEM_PROMPT_MESSAGES = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª Python ç¼–ç¨‹ä¸“å®¶ï¼Œä¸“é—¨å¸®åŠ©åˆå­¦è€…å­¦ä¹ ç¼–ç¨‹ã€‚"},
    {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ Python ä¸­çš„åˆ—è¡¨å’Œå­—å…¸çš„åŒºåˆ«"}
]

# ç¤ºä¾‹ 4: ä»£ç ç”Ÿæˆ
CODE_GENERATION_MESSAGES = [
    {"role": "user", "content": "è¯·å†™ä¸€ä¸ª Python å‡½æ•°ï¼Œè®¡ç®—ä¸¤ä¸ªæ•°çš„æœ€å¤§å…¬çº¦æ•°"}
]

# ç¤ºä¾‹ 5: è‡ªå®šä¹‰æµ‹è¯• - åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„æ¶ˆæ¯
MY_CUSTOM_MESSAGES = [
    {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "},
    {"role": "user", "content": "èƒ½ä¸¾ä¸ªå…·ä½“çš„ä¾‹å­å—ï¼Ÿ"}
]

# ç¤ºä¾‹ 6: æµå¼æµ‹è¯•
STREAM_MESSAGES = [
    {"role": "user", "content": "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²"}
]


async def test_simple_chat():
    """æµ‹è¯•ç®€å•å¯¹è¯"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 1: ç®€å•å¯¹è¯")
    print("=" * 80)

    tester = OpenAIChatTester()
    result = await tester.chat(SIMPLE_MESSAGES)
    return result


async def test_multi_turn_chat():
    """æµ‹è¯•å¤šè½®å¯¹è¯"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 2: å¤šè½®å¯¹è¯")
    print("=" * 80)

    tester = OpenAIChatTester()
    result = await tester.chat(MULTI_TURN_MESSAGES)
    return result


async def test_system_prompt():
    """æµ‹è¯•ç³»ç»Ÿæç¤º"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 3: ç³»ç»Ÿæç¤º")
    print("=" * 80)

    tester = OpenAIChatTester()
    result = await tester.chat(SYSTEM_PROMPT_MESSAGES, temperature=0.5)
    return result


async def test_code_generation():
    """æµ‹è¯•ä»£ç ç”Ÿæˆ"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 4: ä»£ç ç”Ÿæˆ")
    print("=" * 80)

    tester = OpenAIChatTester()
    result = await tester.chat(CODE_GENERATION_MESSAGES, max_tokens=500)
    return result


async def test_custom():
    """æµ‹è¯•è‡ªå®šä¹‰æ¶ˆæ¯"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 5: è‡ªå®šä¹‰æ¶ˆæ¯")
    print("=" * 80)

    tester = OpenAIChatTester()
    result = await tester.chat(MY_CUSTOM_MESSAGES, temperature=0.8)
    return result


async def test_stream():
    """æµ‹è¯•æµå¼è¾“å‡º"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 6: æµå¼è¾“å‡º")
    print("=" * 80)

    tester = OpenAIChatTester()
    result = await tester.chat(STREAM_MESSAGES, stream=True, max_tokens=800)
    return result


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ¯ OpenAI æ¥å£ç›´æ¥æµ‹è¯•è„šæœ¬")
    print("=" * 80)
    print("\nğŸ“Œ ä½¿ç”¨è¯´æ˜:")
    print("1. ä¿®æ”¹ä¸Šé¢çš„ MY_CUSTOM_MESSAGES æ¥æµ‹è¯•ä½ çš„å†…å®¹")
    print("2. è¿è¡Œ: python test_openai.py")
    print("3. æŸ¥çœ‹ OpenAI API çš„ç›´æ¥å“åº”")
    print("\nğŸ“ æ¶ˆæ¯æ ¼å¼:")
    print("   [{\"role\": \"user\", \"content\": \"ä½ çš„é—®é¢˜\"}]")
    print("   æ”¯æŒçš„è§’è‰²: system, user, assistant")

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    test_choice = sys.argv[1] if len(sys.argv) > 1 else "all"
    print(f"\nğŸ” æµ‹è¯•æ¨¡å¼: {test_choice}")

    tests = {
        # "1": ("ç®€å•å¯¹è¯", test_simple_chat),
        "2": ("å¤šè½®å¯¹è¯", test_multi_turn_chat),
        # "3": ("ç³»ç»Ÿæç¤º", test_system_prompt),
        # "4": ("ä»£ç ç”Ÿæˆ", test_code_generation),
        # "5": ("è‡ªå®šä¹‰æ¶ˆæ¯", test_custom),
        # "6": ("æµå¼è¾“å‡º", test_stream),
        # "all": ("æ‰€æœ‰æµ‹è¯•", None)
    }

    if test_choice in tests:
        test_name, test_func = tests[test_choice]
        print(f"\n{'=' * 80}")
        print(f"ğŸ§ª æ­£åœ¨è¿è¡Œ: {test_name}")
        print(f"{'=' * 80}")

        if test_func:
            # è¿è¡Œå•ä¸ªæµ‹è¯•
            result = await test_func()
            if "error" in result:
                print(f"\nâŒ æµ‹è¯•å¤±è´¥: {result['error']}")
            else:
                print(f"\nâœ… æµ‹è¯•æˆåŠŸ")
        else:
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            results = []
            for key, (name, func) in tests.items():
                if key == "all":
                    continue

                print(f"\n\n{'=' * 80}")
                print(f"â–¶ï¸  è¿è¡Œæµ‹è¯• {key}: {name}")
                print(f"{'=' * 80}")

                result = await func()
                results.append((name, "âœ… æˆåŠŸ" if "error" not in result else "âŒ å¤±è´¥"))

                # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                print("\nâ³ ç­‰å¾… 2 ç§’...")
                await asyncio.sleep(2)

            # æ‰“å°æ€»ç»“
            print("\n" + "=" * 80)
            print("ğŸ“Š æµ‹è¯•æ€»ç»“")
            print("=" * 80)
            for name, status in results:
                print(f"{status} - {name}")
    else:
        print(f"\nâŒ æ— æ•ˆçš„æµ‹è¯•é€‰æ‹©: {test_choice}")
        print(f"å¯ç”¨é€‰é¡¹: {', '.join(tests.keys())}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ è„šæœ¬å¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()