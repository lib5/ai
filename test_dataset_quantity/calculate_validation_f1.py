#!/usr/bin/env python3
"""
è®¡ç®—ä¸‰ä¸ªå®Œæ•´éªŒè¯æŠ¥å‘Šçš„F1åˆ†æ•°
åŸºäºæ¨¡å‹åˆ¤æ–­å’ŒçœŸå®é”™è¯¯æ¡ˆä¾‹
"""

import json
import os
from datetime import datetime
from pathlib import Path

def calculate_f1_from_reports():
    """è®¡ç®—ä¸‰ä¸ªéªŒè¯æŠ¥å‘Šçš„F1åˆ†æ•°"""
    print("ğŸ“Š è®¡ç®—ä¸‰ä¸ªéªŒè¯æŠ¥å‘Šçš„F1åˆ†æ•°")
    print("="*100)

    # å®šä¹‰ä¸‰ä¸ªéªŒè¯æŠ¥å‘Š - ä½¿ç”¨åŠ¨æ€æŸ¥æ‰¾
    script_dir = Path(__file__).parent

    def find_latest_report(pattern_name):
        """æŸ¥æ‰¾æœ€æ–°çš„éªŒè¯æŠ¥å‘Š"""
        import glob
        pattern = str(script_dir / "validation_reports" / pattern_name)
        files = glob.glob(pattern)
        if files:
            return max(files, key=os.path.getmtime)
        return None

    def find_error_file(error_name):
        """æŸ¥æ‰¾é”™è¯¯æ–‡ä»¶"""
        error_path = script_dir / "act_erro" / error_name
        return str(error_path) if error_path.exists() else None

    reports = [
        {
            "name": "GeminiéªŒè¯æŠ¥å‘Š",
            "report_path": find_latest_report("validation_report_*gemini*.json"),
            "error_path": find_error_file("gemini_act_erro.json")
        },
        {
            "name": "DoubaoéªŒè¯æŠ¥å‘Š",
            "report_path": find_latest_report("validation_report_*doubao*.json"),
            "error_path": find_error_file("doubao_act_erro.json")
        },
        {
            "name": "QweméªŒè¯æŠ¥å‘Š",
            "report_path": find_latest_report("validation_report_*qwen*.json"),
            "error_path": find_error_file("qwen_act_err.json")
        }
    ]

    results = []

    for report_info in reports:
        print(f"\n{'='*100}")
        print(f"åˆ†æ: {report_info['name']}")
        print(f"{'='*100}")

        # åŠ è½½éªŒè¯æŠ¥å‘Š
        with open(report_info['report_path'], 'r', encoding='utf-8') as f:
            validation_data = json.load(f)

        # åŠ è½½çœŸå®é”™è¯¯æ¡ˆä¾‹
        try:
            with open(report_info['error_path'], 'r', encoding='utf-8') as f:
                error_data = json.load(f)
            real_error_ids = {case['test_case_id'] for case in error_data}
            print(f"çœŸå®é”™è¯¯æ¡ˆä¾‹æ•°: {len(real_error_ids)}")
        except Exception as e:
            real_error_ids = set()
            print(f"æ— æ³•åŠ è½½é”™è¯¯æ•°æ®: {e}")

        # è·å–éªŒè¯æŠ¥å‘Šæ‘˜è¦
        summary = validation_data['validation_summary']
        print(f"æ€»æ¡ˆä¾‹æ•°: {summary['total_cases']}")
        print(f"æ¨¡å‹é¢„æµ‹æ­£ç¡®: {summary['correct_count']}")
        print(f"æ¨¡å‹é¢„æµ‹é”™è¯¯: {summary['wrong_count']}")
        print(f"å‡†ç¡®ç‡: {summary['accuracy_rate']:.2f}%")

        # è®¡ç®—æ··æ·†çŸ©é˜µ
        tp = fp = tn = fn = 0

        for case in validation_data['validation_details']:
            case_id = case['test_case_id']
            model_prediction = case['is_correct']  # æ¨¡å‹çš„é¢„æµ‹
            is_actually_error = case_id in real_error_ids  # å®é™…æƒ…å†µ

            if is_actually_error:
                # å®é™…ä¸Šé”™è¯¯çš„æ¡ˆä¾‹
                if model_prediction == 'é”™è¯¯':
                    tn += 1  # çœŸè´Ÿä¾‹ï¼šæ¨¡å‹é¢„æµ‹é”™è¯¯ï¼Œä¸”ç¡®å®æ˜¯é”™è¯¯
                else:
                    fp += 1  # å‡æ­£ä¾‹ï¼šæ¨¡å‹é¢„æµ‹æ­£ç¡®ï¼Œä½†å®é™…æ˜¯é”™è¯¯
            else:
                # å®é™…ä¸Šæ­£ç¡®çš„æ¡ˆä¾‹
                if model_prediction == 'æ­£ç¡®':
                    tp += 1  # çœŸæ­£ä¾‹ï¼šæ¨¡å‹é¢„æµ‹æ­£ç¡®ï¼Œä¸”ç¡®å®æ˜¯æ­£ç¡®
                else:
                    fn += 1  # å‡è´Ÿä¾‹ï¼šæ¨¡å‹é¢„æµ‹é”™è¯¯ï¼Œä½†å®é™…æ˜¯æ­£ç¡®çš„

        total_cases = len(validation_data['validation_details'])

        # è®¡ç®—æŒ‡æ ‡
        accuracy = (tp + tn) / total_cases if total_cases > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        # ä¿å­˜ç»“æœ
        result = {
            "name": report_info['name'],
            "total_cases": total_cases,
            "true_positive": tp,
            "false_positive": fp,
            "true_negative": tn,
            "false_negative": fn,
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score,
            "real_error_count": len(real_error_ids)
        }
        results.append(result)

        # æ‰“å°ç»“æœ
        print(f"\nğŸ“ˆ F1åˆ†æ•°è®¡ç®—ç»“æœ:")
        print(f"æ··æ·†çŸ©é˜µ:")
        print(f"  TP (çœŸæ­£ä¾‹): {tp}")
        print(f"  FP (å‡æ­£ä¾‹): {fp}")
        print(f"  TN (çœŸè´Ÿä¾‹): {tn}")
        print(f"  FN (å‡è´Ÿä¾‹): {fn}")
        print(f"æ ¸å¿ƒæŒ‡æ ‡:")
        print(f"  F1åˆ†æ•°: {f1_score:.4f} ({f1_score*100:.2f}%)")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"  ç²¾ç¡®ç‡: {precision:.4f} ({precision*100:.2f}%)")
        print(f"  å¬å›ç‡: {recall:.4f} ({recall*100:.2f}%)")

    # å¯¹æ¯”åˆ†æ
    print(f"\n{'='*100}")
    print("ğŸ“Š ä¸‰ä¸ªéªŒè¯æŠ¥å‘Šå¯¹æ¯”")
    print(f"{'='*100}")

    if len(results) >= 2:
        gemini_result = results[0]
        doubao_result = results[1]
        qwen_result = results[2]

        print(f"\nğŸ“‹ å¯¹æ¯”è¡¨æ ¼:")
        print(f"{'æŒ‡æ ‡':<12} | {'Gemini':<15} | {'Doubao':<15} | {'Qwen':<15} | {'æœ€ä½³':<10}")
        print("-" * 85)

        # F1åˆ†æ•°
        f1_scores = [gemini_result['f1_score'], doubao_result['f1_score'], qwen_result['f1_score']]
        best_f1 = max(f1_scores)
        best_model = ['Gemini', 'Doubao', 'Qwen'][f1_scores.index(best_f1)]
        print(f"{'F1åˆ†æ•°':<12} | {gemini_result['f1_score']:<15.4f} | {doubao_result['f1_score']:<15.4f} | {qwen_result['f1_score']:<15.4f} | {best_model:<10}")

        # å‡†ç¡®ç‡
        acc_scores = [gemini_result['accuracy'], doubao_result['accuracy'], qwen_result['accuracy']]
        best_acc = max(acc_scores)
        best_acc_model = ['Gemini', 'Doubao', 'Qwen'][acc_scores.index(best_acc)]
        print(f"{'å‡†ç¡®ç‡':<12} | {gemini_result['accuracy']:<15.4f} | {doubao_result['accuracy']:<15.4f} | {qwen_result['accuracy']:<15.4f} | {best_acc_model:<10}")

        # ç²¾ç¡®ç‡
        prec_scores = [gemini_result['precision'], doubao_result['precision'], qwen_result['precision']]
        best_prec = max(prec_scores)
        best_prec_model = ['Gemini', 'Doubao', 'Qwen'][prec_scores.index(best_prec)]
        print(f"{'ç²¾ç¡®ç‡':<12} | {gemini_result['precision']:<15.4f} | {doubao_result['precision']:<15.4f} | {qwen_result['precision']:<15.4f} | {best_prec_model:<10}")

        # å¬å›ç‡
        recall_scores = [gemini_result['recall'], doubao_result['recall'], qwen_result['recall']]
        best_recall = max(recall_scores)
        best_recall_model = ['Gemini', 'Doubao', 'Qwen'][recall_scores.index(best_recall)]
        print(f"{'å¬å›ç‡':<12} | {gemini_result['recall']:<15.4f} | {doubao_result['recall']:<15.4f} | {qwen_result['recall']:<15.4f} | {best_recall_model:<10}")

        # è®¡ç®—å¹³å‡F1ï¼ˆä¸‰ä¸ªæ¨¡å‹ï¼‰
        avg_f1 = (gemini_result['f1_score'] + doubao_result['f1_score'] + qwen_result['f1_score']) / 3
        print(f"\nğŸ¯ ä¸‰ä¸ªæ¨¡å‹çš„å¹³å‡F1åˆ†æ•°: {avg_f1:.4f} ({avg_f1*100:.2f}%)")
        print(f"   Gemini F1: {gemini_result['f1_score']:.4f} ({gemini_result['f1_score']*100:.2f}%)")
        print(f"   Doubao F1: {doubao_result['f1_score']:.4f} ({doubao_result['f1_score']*100:.2f}%)")
        print(f"   Qwen F1:   {qwen_result['f1_score']:.4f} ({qwen_result['f1_score']*100:.2f}%)")

        # åˆ†ææœ€ä½³æ¨¡å‹
        print(f"\nğŸ† æ€§èƒ½æ’å:")
        sorted_results = sorted(zip(['Gemini', 'Doubao', 'Qwen'], f1_scores), key=lambda x: x[1], reverse=True)
        for i, (model, score) in enumerate(sorted_results, 1):
            print(f"   {i}. {model}: {score:.4f} ({score*100:.2f}%)")

        # æ€§èƒ½å·®å¼‚åˆ†æ
        f1_range = max(f1_scores) - min(f1_scores)
        if f1_range < 0.01:
            print(f"   âœ… ä¸‰ä¸ªæ¨¡å‹æ€§èƒ½æ¥è¿‘ (F1èŒƒå›´: {f1_range:.4f})")
        else:
            best_model_name = sorted_results[0][0]
            print(f"   ğŸ“ˆ {best_model_name}æ€§èƒ½æœ€ä½³ï¼Œé¢†å…ˆ {f1_range:.4f}")

    # åˆ›å»ºf1_scoreç›®å½•
    f1_score_dir = script_dir / "f1_score"
    os.makedirs(f1_score_dir, exist_ok=True)

    # ä¿å­˜ç»“æœ
    output_file = f1_score_dir / f"validation_f1_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "results": results,
            "average_f1": avg_f1 if len(results) >= 3 else None,
            "description": "ä¸‰ä¸ªéªŒè¯æŠ¥å‘Šçš„F1åˆ†æ•°åˆ†æ"
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    calculate_f1_from_reports()
